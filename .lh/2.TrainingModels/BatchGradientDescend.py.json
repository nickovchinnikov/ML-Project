{
    "sourceFile": "2.TrainingModels/BatchGradientDescend.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1653123210673,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1653123573949,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import numpy as np\n from sklearn import datasets\n from sklearn.linear_model import LogisticRegression\n from sklearn.metrics import accuracy_score\n-\n+bv\n # %%\n # Load the iris dataset\n iris = datasets.load_iris()\n list(iris.keys())\n"
                },
                {
                    "date": 1653296394043,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import numpy as np\n from sklearn import datasets\n from sklearn.linear_model import LogisticRegression\n from sklearn.metrics import accuracy_score\n-bv\n+\n # %%\n # Load the iris dataset\n iris = datasets.load_iris()\n list(iris.keys())\n@@ -155,9 +155,9 @@\n theta = training(y_k, x, theta, n_iterations, eta)\n theta\n \n # %% [markdown]\n-\"\"\"\n+r\"\"\"\n #### Equation 4-21. Softmax Regression classifier prediction\n \n $$\n \\hat{y} = \n"
                },
                {
                    "date": 1653296419145,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,9 +155,9 @@\n theta = training(y_k, x, theta, n_iterations, eta)\n theta\n \n # %% [markdown]\n-r\"\"\"\n+\"\"\"\n #### Equation 4-21. Softmax Regression classifier prediction\n \n $$\n \\hat{y} = \n"
                },
                {
                    "date": 1653296427624,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,31 +155,31 @@\n theta = training(y_k, x, theta, n_iterations, eta)\n theta\n \n # %% [markdown]\n-\"\"\"\n+\n #### Equation 4-21. Softmax Regression classifier prediction\n \n-$$\n-\\hat{y} = \n-\\underset{k}{\\operatorname{argmax}} \\space \\sigma(s(x))_k\n-$$\n+# $$\n+# \\hat{y} = \n+# \\underset{k}{\\operatorname{argmax}} \\space \\sigma(s(x))_k\n+# $$\n \n-Because $\\sigma(s(x))_k$ is the ration we can use\n+# Because $\\sigma(s(x))_k$ is the ration we can use\n \n-$$\n-\\hat{y} =\n-\\underset{k}{\\operatorname{argmax}} \\space s_k(x)\n-$$\n+# $$\n+# \\hat{y} =\n+# \\underset{k}{\\operatorname{argmax}} \\space s_k(x)\n+# $$\n \n-Replace $s_k(x)$\n+# Replace $s_k(x)$\n \n-$$\n-\\hat{y} =\n-\\underset{k}{\\operatorname{argmax}} \\space ((\\theta^{(k)})^T x)\n-$$\n-\"\"\"\n+# $$\n+# \\hat{y} =\n+# \\underset{k}{\\operatorname{argmax}} \\space ((\\theta^{(k)})^T x)\n+# $$\n \n+\n # %%\n # Check on the validation set\n Y_proba = softmax(X_valid, theta)\n y_predict = np.argmax(Y_proba, axis=1)\n"
                }
            ],
            "date": 1653123210673,
            "name": "Commit-0",
            "content": "# %%\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# %%\n# Load the iris dataset\niris = datasets.load_iris()\nlist(iris.keys())\n\n# %%\n# Description of the dataset\nprint(iris.DESCR)\n\n# %% [markdown]\n\"\"\"\nLoadings the iris dataset into the variables X and y.\n\"\"\"\n\n# %%\nX = iris[\"data\"][:, 2:] # petal length and width\ny = iris[\"target\"]\n\nX[:5, :], y[:5]\n\n# %% [markdown]\n\"\"\"\nWe need to add bias term for every instance $x_0 = 1$\n\"\"\"\n\n# %%\nX_with_bias = np.c_[np.ones((len(X), 1)), X]\n\nX_with_bias[:5, :]\n\n# %% [markdown]\n\"\"\"\nSet the seed to 2042 to get the same random numbers every time so the output will be the same every time.\n\"\"\"\n\n# %%\nnp.random.seed(2042)\n\n# %% [markdown]\n\"\"\"\nManual implemetation train_test_split\n\"\"\"\n# %%\ntest_ratio = 0.2\nvalidation_ratio = 0.2\n\ntotal_size = len(X_with_bias)\n\ntest_size = int(total_size * test_ratio)\nvalidation_size = int(total_size * validation_ratio)\ntrain_size = total_size - test_size - validation_size\n\nrnd_indices = np.random.permutation(total_size)\n\nX_train = X_with_bias[rnd_indices[:train_size]]\ny_train = y[rnd_indices[:train_size]]\n\nX_valid = X_with_bias[rnd_indices[train_size:-test_size]]\ny_valid = y[rnd_indices[train_size:-test_size]]\n\nX_test = X_with_bias[rnd_indices[-test_size:]]\ny_test = y[rnd_indices[-test_size:]]\n\n# %%\ndef to_one_hot(y):\n    n_classes = y.max() + 1\n    m = len(y)\n    Y_one_hot = np.zeros((m, n_classes))\n    Y_one_hot[np.arange(m), y] = 1\n    return Y_one_hot\n\n# %%\n# First 10 instances\ny_train[:10]\n\n# %%\n# Test to_one_hot to the first 10 instances\nto_one_hot(y_train[:10])\n\n# %%\n# Let's create the target class probabilities for the test set and the training set\nY_train_one_hot = to_one_hot(y_train)\nY_test_one_hot = to_one_hot(y_test)\nY_valid_one_hot = to_one_hot(y_valid)\n\n# %% [markdown]\n\"\"\"\nLet's implement the softmax regression function by the following equation:\n\n$$s_k(x)=x^T\\theta^{(k)}$$\n\n$$\\hat{p_k} = \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$$\n\"\"\"\n\n# %%\ndef softmax(x, theta):\n    logits = x @ theta\n    exps = np.exp(logits)\n    exp_sums = np.sum(exps, axis=1, keepdims=True)\n    return exps / exp_sums\n\n# %%\nn_inputs = X_train.shape[1]\nn_outputs = len(np.unique(y_train))\n\nn_inputs, n_outputs\n\n# %% [markdown]\n\"\"\"\nSo the equations we will need are the cost (entropy loss) function:\n\n$$J(\\mathbf{\\Theta}) = - \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$$\n\nAnd the equation for the gradients:\n\n$$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$$\n\nNote that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\\epsilon$ to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting `nan` values.\n\"\"\"\n\n# %%\n# Machine eps\neps = np.finfo(np.float32).eps\n\ndef training(y_k, x, theta, n_iterations, eta, epsilon = eps):\n    m = len(x)\n    for iteration in range(n_iterations):\n        p_k = softmax(X_train, theta)\n\n        if iteration % 500 == 0:\n            entropy_loss = -np.mean(np.sum(y_k * np.log(p_k + epsilon), axis=1))\n            print(iteration, entropy_loss)\n\n        grad = (1/m) * x.T @ (p_k - y_k)\n        theta = theta - eta * grad\n    return theta\n\n# %%\n# Assign values to named variables\ny_k = Y_train_one_hot\nx = X_train\n# For the batch gradient it should be the random batch\ntheta = np.random.randn(n_inputs, n_outputs)\n# Additional parameters fot the training\nn_iterations = 5001\neta = 0.01\n\ntheta = training(y_k, x, theta, n_iterations, eta)\ntheta\n\n# %% [markdown]\n\"\"\"\n#### Equation 4-21. Softmax Regression classifier prediction\n\n$$\n\\hat{y} = \n\\underset{k}{\\operatorname{argmax}} \\space \\sigma(s(x))_k\n$$\n\nBecause $\\sigma(s(x))_k$ is the ration we can use\n\n$$\n\\hat{y} =\n\\underset{k}{\\operatorname{argmax}} \\space s_k(x)\n$$\n\nReplace $s_k(x)$\n\n$$\n\\hat{y} =\n\\underset{k}{\\operatorname{argmax}} \\space ((\\theta^{(k)})^T x)\n$$\n\"\"\"\n\n# %%\n# Check on the validation set\nY_proba = softmax(X_valid, theta)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy = (y_valid == y_predict).astype(np.int32)\n\naccuracy_score = np.mean(accuracy)\naccuracy_score\n\n# %% [markdown]\n\"\"\"\nCreate traning function with $l_2$ regularization\n\n#### Equation 4-8. Ridge Regression cost function with $l_2$ regularization\n\n$$\nJ(\\theta)=MSE(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n} {\\theta_i^2}\n$$\n\"\"\"\n\n# %%\n# $l_2$ loss example\nl2_loss = 1/2 * np.sum(np.square(theta[1:]))\nl2_loss\n\n# %%\ndef training_reg_l2(y_k, x, theta, n_iterations, eta, alpha, epsilon = eps):\n    m = len(x)\n    for iteration in range(n_iterations):\n        p_k = softmax(X_train, theta)\n\n        if iteration % 500 == 0:\n            xentropy_loss = -np.mean(np.sum(y_k * np.log(p_k + epsilon), axis=1))\n            l2_loss = alpha * 1/2 * np.sum(np.square(theta[1:]))\n            entropy_loss = xentropy_loss + l2_loss\n            print(iteration, entropy_loss)\n\n        # additional l_2 parameter\n        reg = np.r_[np.zeros([1, n_outputs]), 0.1 * theta[1:]]\n        grad = (1/m) * x.T @ (p_k - y_k) + reg\n        theta = theta - eta * grad\n    return theta\n\n# %%\n# Assign values to named variables\ny_k = Y_train_one_hot\nx = X_train\n# For the batch gradient it should be the random batch\ntheta = np.random.randn(n_inputs, n_outputs)\n# Additional parameters fot the training\nn_iterations = 5001\neta = 0.1\nalpha = 0.1\n\ntheta = training_reg_l2(y_k, x, theta, n_iterations, eta, alpha)\ntheta\n\n# %%\n# Check performance of the model\nY_proba = softmax(X_valid, theta)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy = (y_valid == y_predict).astype(np.int32)\n\naccuracy_score = np.mean(accuracy)\naccuracy_score\n\n# %%\n# Let's add early stopping\ndef training_l2_early_stop(y_k, x, theta, n_iterations, eta, alpha, r_num=6, epsilon = eps):\n    best_loss = np.infty\n    m = len(x)\n    for iteration in range(n_iterations):\n        p_k = softmax(X_train, theta)\n\n        # additional l_2 parameter\n        reg = np.r_[np.zeros([1, n_outputs]), 0.1 * theta[1:]]\n        grad = (1/m) * x.T @ (p_k - y_k) + reg\n        theta = theta - eta * grad\n\n        xentropy_loss = -np.mean(np.sum(y_k * np.log(p_k + epsilon), axis=1))\n        l2_loss = alpha * 1/2 * np.sum(np.square(theta[1:]))\n        entropy_loss = xentropy_loss + l2_loss\n\n        if iteration % 500 == 0:\n            print(iteration, entropy_loss)\n        \n        if round(entropy_loss, r_num) < round(best_loss, r_num):\n            best_loss = entropy_loss\n        else:\n            print(iteration - 1, best_loss)\n            print(iteration, entropy_loss, \"early stopping!\")\n            return theta\n\n    return theta\n\n# %%\n# Assign values to named variables\ny_k = Y_train_one_hot\nx = X_train\n# For the batch gradient it should be the random batch\ntheta = np.random.randn(n_inputs, n_outputs)\n# Additional parameters fot the training\nn_iterations = 5001\neta = 0.1\nalpha = 0.1\n\ntheta = training_l2_early_stop(y_k, x, theta, n_iterations, eta, alpha)\ntheta\n\n# %%\n# Check performance of the model\nY_proba = softmax(X_valid, theta)\ny_predict = np.argmax(Y_proba, axis=1)\n\naccuracy = (y_valid == y_predict).astype(np.int32)\n\naccuracy_score = np.mean(accuracy)\naccuracy_score\n\n# %%\n# Plot the model of the whole prediction\nx0, x1 = np.meshgrid(\n    np.linspace(0, 8, 500).reshape(-1, 1),\n    np.linspace(0, 3.5, 200).reshape(-1, 1),\n)\nX_new = np.c_[x0.ravel(), x1.ravel()]\nX_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n\nY_proba = softmax(X_new_with_bias, theta)\ny_predict = np.argmax(Y_proba, axis=1)\n\nzz1 = Y_proba[:, 1].reshape(x0.shape)\nzz = y_predict.reshape(x0.shape)\n\nplt.figure(figsize=(10, 4))\nplt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n\nplt.contourf(x0, x1, zz, cmap=custom_cmap)\ncontour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\nplt.clabel(contour, inline=1, fontsize=12)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 7, 0, 3.5])\nplt.show()\n# %%\n"
        }
    ]
}